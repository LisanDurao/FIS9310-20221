{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is one of the most powerful techniques devloped in modern machine learning. It applies for both regression and classification, and despite the similarity with bagging, is a trully different method. It is an ensemble method formed only of weak learners, in general trees, combine in a sequential manner, so that the combination results in a strong learner. \n",
    "\n",
    "A weak learner is a predictor that is just slightly better than a random guessing. The idea of boosting is to sequentially apply the weak classification algorithm to a repeteadly modified version of the dataset producing a sequence of weak classifiers $G_m(\\vec{x}),\\: m=1,2,...,M.$ The modified version arise from the weighted combination of the predictors:\n",
    "\n",
    "$$\n",
    "G(\\vec{x}) = \\sum_{m=1}^M \\alpha_m G_m(\\vec{x})\n",
    "$$\n",
    "\n",
    "in which the weights $\\alpha_m$ are defined by the boosting algorithm in such manner that the instances that are harder to predict receives more atention than the ones that are easier. The modification address weights not only to the predictors but to the instances $(x_i,y_i), \\: i=1,2,...,N$. All instances are initialized with equal weights $w_i = 1/N$. In the following executions, they are re-weighted so that the harder to predict, the sequential prediction focused in the higher weighted instances, so that the $G_i(\\vec{x})$ attempts to correct the errors of the $G_{i-1}(\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why boosting is a success isn't not difficult to figure out. They key issue is that boosting is a way of fitting an additive expansion in a set of elementary \"basis\" function where the weak learner does the job of basis. In this sense, we recast the boosting definition as:\n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\sum_{m=1}^M \\beta_mb(\\vec{x},\\gamma_m),\n",
    "$$\n",
    "\n",
    "where $\\beta_m, \\: m=1,2,...,M$ are the expansion coefficients, and $b(\\vec{x},\\gamma_m) \\in \\mathbb{R}$ are some algebraic simple functions of the multivariate feature vector, characterized by the set of parameters $\\gamma$. They are the heart and soul of several learning algorithms:\n",
    "\n",
    "* Single-hidden-layer neural networks: $b(\\vec{x},\\gamma) = \\sigma(\\gamma_0 + \\vec{\\gamma}_1^T\\cdot\\vec{x} )$, where $\\sigma(\\cdot)$ is the sigmoid function.\n",
    "\n",
    "* Multivariate adaptative regression splines, uses truncated power splines basis function where $\\sigma$ parametrizes the variables and values for the knots.\n",
    "\n",
    "* For trees, the parameters $\\gamma$ defines the split variables and the split points at the internal nodes and the predictions of the terminal nodes.\n",
    "\n",
    "As usual, these models are fitted by minimizing a loss function averaged over the training data using likelihood-based loss function:\n",
    "\n",
    "$$\n",
    "min_{\\{\\beta_m,\\gamma_m\\}_1^M}\\sum_{i=1}^N L\\left(y_i,\\sum_{m=1}^M\\beta_mb(\\vec{x}_i,\\gamma_m)\\right),\n",
    "$$\n",
    "\n",
    "For many loss functions, the likelihood and/or basis functions, this requeires computationally intensive numerical optimization techniques. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward Stagewise, Addtive Modeling, Exponential Loss and AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible solution for the optimization problem is to sequentialy add new basis functions to the expansion, without adjusting the parameters and coefficients of those that have alredy been added. At each the $m-$th iteration, the algorithm solves for the optimal basis function $b(\\vec{x},\\gamma_m)$ and corresponding coefficient $\\beta_m$ to the current expansion $f_{m-1}(\\vec{x})$, producing the $f_m(\\vec{x})$. The proces is thus repeated until convergence, and previously added terms are not modified. For the squared-loss function, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(y_i,f_{m-1}(\\vec{x})+\\beta b(\\vec{x},\\gamma)) &= (y_i - f_{m-1}(\\vec{x}) - \\beta b(\\vec{x},\\gamma))^2 \\\\\n",
    "                                                &= (r_{im} - \\beta b(\\vec{x},\\gamma))^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "thus, at each step, the optimization procedure takes as input the residue of the $m-1$ estimation. The new basis function that best fit the residues is included in the model expansion. This foward stagewise procedure can be applied to any kind of acceptable loss function. Each case corresponds to an algorithm. The seminal boosting method, Adaptative Boosting, AdaBoost for short, can be derived in this way, using an exponential loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the so-called exponential loss function:\n",
    "\n",
    "$$\n",
    "L(y,f(\\vec{x})) = exp(-y\\times f(\\vec{x})),\n",
    "$$\n",
    "and we assume the basis function to be individual predictors $G_m(\\vec{x})$. One must solve:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\beta_m,G_m) &= arg min_{\\beta,G}\\sum_{i=1}^N exp\\left[-y_i(f_{m-1}(\\vec{x})+\\beta G(\\vec{x}))\\right]\\\\\n",
    "              &= arg min_{\\beta,G}\\sum_{i=1}^N w_i^{(m)}exp\\left[-y_i(\\beta G(\\vec{x}))\\right],\n",
    "\\end{aligned}\n",
    "$$\n",
    "since $w_i^{(m)} = exp\\left[-y_i(f_{m-1}(\\vec{x}))\\right]$ does not depends on neither $\\beta$ nor $G(\\vec{x})$ it can be factor out as a weight factor applied to each instance. For a classification problem, the $m-th$ $\\beta$ is obtained as:\n",
    "\n",
    "$$\n",
    "err_{m} = \\frac{\\sum_{i=1}^Nw_{i}^{(m)}I(y_i \\neq G_m(\\vec{x}))}{\\sum_{i=1}^Nw_i^{(m)}}.\n",
    "$$\n",
    "\n",
    "The foward prediction is given by:\n",
    "\n",
    "$$\n",
    "f_m(\\vec{x}) = f_{m-1}(\\vec{x}) + \\beta_{m}G_m(\\vec{x}),\n",
    "$$\n",
    "which results into the following update rule:\n",
    "\n",
    "$$\n",
    "w_i^{(m+1)} = w_i^{(m)}exp(2\\beta_mI(y_i \\neq G_m(\\vec{x})) - \\beta_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a reminder, classification and regression trees, as discussed before, are partitions of the feature space off all joint independent variables into disjoint regions, represented as the terminal nodes in which a prediction is made, based on the average of the region. Thus, a tree model partition the space and assign to each partition a constant value:\n",
    "\n",
    "$$\n",
    "x \\in R_j \\longrightarrow f(x) = \\gamma_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tree can be formally expressed as:\n",
    "\n",
    "$$\n",
    "T(\\vec{x},\\Theta) = \\sum_{j=1}^J\\gamma_jI(\\vec{x} \\ in R_j),\n",
    "$$\n",
    "where $\\Theta = \\{R_j,\\gamma_j\\}_ 1^J$. In general, $J$ represent all possible combinations of tree hyperparameters and are determined by a combination os cross-validation and empirical risk minimization:\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta} = argmin_{\\Theta}\\sum_{j=1}^J\\sum_{x_i\\in R_j}L(y_i, \\gamma_j)\n",
    "$$\n",
    "\n",
    "This is a complicated optimization problem and in general we relies upon suboptimal solutions. We divide this problem into two parts. Forest of all, we find $\\gamma_j$ given $R_j$. This typically trivial and in general we set $\\gamma_j = \\bar{y}_j$. Than we follow to the second part, finding $R_j$ itself. This is the difficult part. In the tree study, we relied upon greedy solutions under the CART method. Finding $R_j$ requires a first estimation of $\\gamma_j$ itself.\n",
    "\n",
    "Boosted tree models is a sum of these trees:\n",
    "\n",
    "$$\n",
    "f_M(\\vec{x}) = \\sum_{m=1}^M T(\\vec{x},\\Theta_m),\n",
    "$$\n",
    "using the forward stagewise approach. At each step, one must solve:\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta}_m  = arg min_{\\Theta_m}\\sum_{i=1}^N L(y_im f_{m-1}(\\vec{x}) + T(\\vec{x},\\Theta_m)).\n",
    "$$\n",
    "\n",
    "Once the region and the hyperparameters were found, one can simply determine the stage prediction:\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}_{jm} = argmin_{\\gamma_{jm}}\\sum_{x_j\\in R_j}L(y_i, f_{m-i}(x_i) + \\gamma_{jm}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, our next step is moving beyond the greedy solution for these tree problems and unleash the true power of trees and boosting using numerical optimization. The idea is to use a fast approximate algorithm to solve the empirical minimization problem:\n",
    "$$\n",
    "L(f) = \\sum_{i=1}^NL(y_i,f(x_i)),\n",
    "$$\n",
    "with respect to a f function that is a weighted sum of trees, assuming that the parameters are the trees itself:\n",
    "\n",
    "$$\n",
    "\\hat{f} = arg min_{f}L(f) \\longrightarrow d = \\{f(\\vec{x}_1),...,f(\\vec{x}_N)\\}.\n",
    "$$\n",
    "The solution here has to be more sophisticated than the approach used to loss function and maximum likelihood. We need a numerical optimization method. The most used one is the so-called Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gradient descent is a generic numerical optimization method used to find solutions to a wide range of problems by tweaking parameters iteratively towards a minimization of cost function. The fundamental aspect of Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. A too small learning rate, means that the algorithm has to go many interactions to converge whereas if the learning rate is too high, you might jump across the potential landscape without never reaching a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In machine learning, optimization methods plays a fundamental role even performing one single task: minimize the cost function: $E(\\theta) = \\mathcal{C}(\\vec{x},f(\\vec{x},\\theta))$. The simple Gradient Descent approach, we update the parameters using the following rule:\n",
    "$$\n",
    "\\vec{v}_t = \\eta_t \\nabla_{\\theta}E(\\theta_t),\n",
    "$$\n",
    "$$\n",
    "\\vec{\\theta}_{t+1} = \\vec{\\theta}_t - \\vec{v}_t,\n",
    "$$\n",
    "where $\\nabla_{\\theta}E(\\theta_t)$ is the gradient of the cost function and $\\eta_t$ is the learning rate. For a sufficient small choice of the learning rate, the algorithm will converge to a local minimum of the cost function. However, this choice comes with the tradeoff of a high computational cost, since the smaller the $\\eta_t$ more steps we have to take to reach a solution. The overshooting choice, large learning rate, brings instabilities to the algorithm and it may never converge. Despite of it conceptual simplicity and practice, Gradient Descent has some limitations that needs to be accounted for:\n",
    "\n",
    "1. Gradient Descent is a deterministic algorithm. This implies that if ti converges, it will converge for a local minimum. In machine learning we often deals with landscapes with many local minimum, so Gradient Descent can lead to poor performance.\n",
    "2. Gradient are computationally cost for large datasets\n",
    "3. Gradient Descent is sensitive to choices of the learning rates.\n",
    "4. Treats all directions in parameter space uniformly\n",
    "5. Sensitive to initial conditions\n",
    "6. Take exponential time to scape saddles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many of these shortcomings can be addressed by a few variants of Gradient Descent. The most famous one is the so-called Stochastic Gradient Descent. One cand introduce stochasticity into gradient descent by approximating the gradient in a subset of data called minibatch. This subset is always much smaller than the total number of datapoints. Thus, in this approach, each gradient descent step, we approximate the gradient using a minibatch $B_k$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}E(\\theta) = \\sum_{i=1}^n\\nabla_{\\theta}e_i(\\vec{x}_i,\\theta) \\longrightarrow \\sum_{i \\in B_k}^n\\nabla_{\\theta}e_i(\\vec{x}_i,\\theta).\n",
    "$$\n",
    "\n",
    "We then cycle over all $k= 1, ..., n/M$ minibatches one at time, and use the mini-batch approximation to the gradient ot update the parameters $\\theta$ at every step $k$. We call a full iteration an epoch. We write the SGD algorithm over the minibatche procedure as:\n",
    "\n",
    "$$\n",
    "\\vec{v}_t = \\eta_t\\nabla_{\\theta}E^{MB}(\\theta),\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\vec{v}_t.\n",
    "$$\n",
    "\n",
    "The Stochastic Gradient Algorithm we replace the gradient over the full data at each step by an approximation calculated using the minibatch. The stochasticity is introduced by the sampling over tha mini batches, reducing the chance of the algorithm gets stuck in isolated local minima and speeds up the calculation as one does not need to use all datapoints. Also, the stochasticity reduces the chance of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the model evaluation commands\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions That Evaluate Model Performance\n",
    "##########################- Root Mean Square -####################################\n",
    "def rmse(model, x_val, y_val):\n",
    "    pred = model.predict(x_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, pred))\n",
    "##########################- R2 Score -###########################################\n",
    "def r2(model, x_val, y_val):\n",
    "    pred = model.predict(x_val)\n",
    "    return r2_score(y_val, pred)\n",
    "##########################- Accuracy -###########################################\n",
    "def accuracy(model, x_val, y_val):\n",
    "    pred = model.predict(x_val)\n",
    "    return r2_score(y_val, pred)\n",
    "##########################- Precision -##########################################\n",
    "def precision(model, x_val, y_val):\n",
    "    pred = model.predict(x_val)\n",
    "    return r2_score(y_val, pred)\n",
    "##########################- Recall -#############################################\n",
    "def recall(model, x_val, y_val):\n",
    "    pred = model.predict(x_val)\n",
    "    return r2_score(y_val, pred)\n",
    "##########################- Classification Repor -###############################\n",
    "def report(model, x_val, y_val):\n",
    "    pred = model.predict(x_val)\n",
    "    return r2_score(y_val, pred)\n",
    "##########################- ROC -################################################\n",
    "def roc(model, x_val, y_val):\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "    # predict probabilities\n",
    "    lr_probs = dt.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Model: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='Model')\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    return plt.show()\n",
    "##########################- Validation Plot -####################################\n",
    "def valid_reg(model, x_val, y_val):\n",
    "    # Place an orange \"X\" on the point with the best Sharpe ratio\n",
    "    pred = model.predict(x_val)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.scatter(x=y_val, y=pred,alpha=0.5,color='green')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax.add_line(line)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Target')\n",
    "    plt.ylabel('Prediction')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant  season   yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0        1     1.0  0.0   1.0      0.0      6.0         0.0           2   \n",
       "1        2     1.0  0.0   1.0      0.0      0.0         0.0           2   \n",
       "2        3     1.0  0.0   1.0      0.0      1.0         1.0           1   \n",
       "3        4     1.0  0.0   1.0      0.0      2.0         1.0           1   \n",
       "4        5     1.0  0.0   1.0      0.0      3.0         1.0           1   \n",
       "\n",
       "       temp     atemp       hum  windspeed   cnt  \n",
       "0  0.344167  0.363625  0.805833   0.160446   985  \n",
       "1  0.363478  0.353739  0.696087   0.248539   801  \n",
       "2  0.196364  0.189405  0.437273   0.248309  1349  \n",
       "3  0.200000  0.212122  0.590435   0.160296  1562  \n",
       "4  0.226957  0.229270  0.436957   0.186900  1600  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bikes = pd.read_csv('bike_rentals_cleaned.csv')\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize Decision Tree Regressor\n",
    "tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "\n",
    "# Fit tree to training data\n",
    "tree_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on training set\n",
    "y_train_pred = tree_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute residuals\n",
    "y2_train = y_train - y_train_pred\n",
    "\n",
    "# Initialize Decision Tree Regressor\n",
    "tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "\n",
    "# Fit tree to training data\n",
    "tree_2.fit(X_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on training set\n",
    "y2_train_pred = tree_2.predict(X_train)\n",
    "\n",
    "# Compute residuals\n",
    "y3_train = y2_train - y2_train_pred\n",
    "\n",
    "# Initialize Decision Tree Regressor\n",
    "tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "\n",
    "# Fit tree to training data\n",
    "tree_3.fit(X_train, y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911.0479538776444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_pred = tree_1.predict(X_test)\n",
    "\n",
    "y2_pred = tree_2.predict(X_test)\n",
    "\n",
    "y3_pred = tree_3.predict(X_test)\n",
    "\n",
    "y_pred = y1_pred + y2_pred + y3_pred\n",
    "\n",
    "# Import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute root mean squared error (rmse)\n",
    "MSE(y_test, y_pred)**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on training set: 801.1808431544883\n",
      "R2 on training set: 0.8259707682214588\n",
      "rmse on test set: 911.0479538776439\n",
      "R2 on test set: 0.7873491063875538\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Compute root mean squared error (rmse)\n",
    "#MSE(y_test, y_pred)**0.5\n",
    "print(f\"rmse on training set: {rmse(gbr, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(gbr, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(gbr,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(gbr,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on training set: 437.00944371671716\n",
      "R2 on training set: 0.9482223024358075\n",
      "rmse on test set: 857.1072323426944\n",
      "R2 on test set: 0.8117846423175021\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "# Compute root mean squared error (rmse)\n",
    "#MSE(y_test, y_pred)**0.5\n",
    "print(f\"rmse on training set: {rmse(gbr, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(gbr, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(gbr,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(gbr,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on training set: 38.12518194485307\n",
      "R2 on training set: 0.9996059195049195\n",
      "rmse on test set: 936.3617413678853\n",
      "R2 on test set: 0.7753677748365476\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(f\"rmse on training set: {rmse(gbr, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(gbr, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(gbr,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(gbr,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on training set: 328.6357792505077\n",
      "R2 on training set: 0.9707186713820268\n",
      "rmse on test set: 653.7456840231495\n",
      "R2 on test set: 0.890502952905042\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(f\"rmse on training set: {rmse(gbr, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(gbr, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(gbr,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(gbr,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001 , Score: 1633.0261400367258\n",
      "Learning Rate: 0.01 , Score: 831.5430182728547\n",
      "Learning Rate: 0.05 , Score: 685.0192988749717\n",
      "Learning Rate: 0.1 , Score: 653.7456840231495\n",
      "Learning Rate: 0.15 , Score: 687.666134269379\n",
      "Learning Rate: 0.2 , Score: 664.312804425697\n",
      "Learning Rate: 0.3 , Score: 689.4190385930236\n",
      "Learning Rate: 0.5 , Score: 693.8856905068778\n",
      "Learning Rate: 1.0 , Score: 936.3617413678853\n"
     ]
    }
   ],
   "source": [
    "learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
    "for value in learning_rate_values:\n",
    "    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=value)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Learning Rate:', value, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: None , Score: 869.2788645118395\n",
      "Max Depth: 1 , Score: 707.8261886858736\n",
      "Max Depth: 2 , Score: 653.7456840231495\n",
      "Max Depth: 3 , Score: 646.4045923317708\n",
      "Max Depth: 4 , Score: 663.048387855927\n"
     ]
    }
   ],
   "source": [
    "depths = [None, 1, 2, 3, 4]\n",
    "for depth in depths:\n",
    "    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Max Depth:', depth, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsample: 1 , Score: 646.4045923317708\n",
      "Subsample: 0.9 , Score: 620.1819001443569\n",
      "Subsample: 0.8 , Score: 617.2355650565677\n",
      "Subsample: 0.7 , Score: 612.9879156983139\n",
      "Subsample: 0.6 , Score: 622.6385116402317\n",
      "Subsample: 0.5 , Score: 626.9974073227554\n"
     ]
    }
   ],
   "source": [
    "samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "for sample in samples:\n",
    "    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=300, subsample=sample, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Subsample:', sample, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.65, 'n_estimators': 300, 'learning_rate': 0.05}\n",
      "Training score: 636.200\n",
      "rmse on training set: 269.10852165028706\n",
      "R2 on training set: 0.9803656742918951\n",
      "rmse on test set: 625.9849010532475\n",
      "R2 on test set: 0.8996049144921957\n"
     ]
    }
   ],
   "source": [
    "params={'subsample':[0.65, 0.7, 0.75],\n",
    "        'n_estimators':[300, 500, 1000],\n",
    "        'learning_rate':[0.05, 0.075, 0.1]\n",
    "        }\n",
    "\n",
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=3, random_state=2)\n",
    "\n",
    "\n",
    "# Instantiate RandomizedSearchCV as rand_reg\n",
    "rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error',\n",
    "                              cv=5, n_jobs=-1, random_state=2)\n",
    "\n",
    "# Fit grid_reg on X_train and y_train\n",
    "rand_reg.fit(X_train, y_train)\n",
    "\n",
    "# Extract best estimator\n",
    "best_model = rand_reg.best_estimator_\n",
    "\n",
    "# Extract best params\n",
    "best_params = rand_reg.best_params_\n",
    "\n",
    "# Print best params\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Compute best score\n",
    "best_score = np.sqrt(-rand_reg.best_score_)\n",
    "\n",
    "# Print best score\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"rmse on training set: {rmse(best_model, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(best_model, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(best_model,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(best_model,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on training set: 159.90255545058218\n",
      "R2 on training set: 0.9930677869910395\n",
      "rmse on test set: 596.9544588974487\n",
      "R2 on test set: 0.9087007649429504\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(f\"rmse on training set: {rmse(gbr, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(gbr, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(gbr,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(gbr,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on training set: 180.53055516221195\n",
      "R2 on training set: 0.99116386127466\n",
      "rmse on test set: 584.3395337495713\n",
      "R2 on test set: 0.9125186901052338\n"
     ]
    }
   ],
   "source": [
    "# Import XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Instantiate the XGBRegressor, xg_reg\n",
    "xg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)\n",
    "\n",
    "# Fit xg_reg to training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels of test set, y_pred\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute root mean squared error (rmse)\n",
    "print(f\"rmse on training set: {rmse(xg_reg, X_train, y_train)}\")\n",
    "print(f\"R2 on training set: {r2(xg_reg, X_train, y_train)}\")\n",
    "print(f\"rmse on test set: {rmse(xg_reg,X_test,y_test)}\")\n",
    "print(f\"R2 on test set: {r2(xg_reg,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}